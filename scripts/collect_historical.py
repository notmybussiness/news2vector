"""
ê³¼ê±° ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ë„¤ì´ë²„ APIëŠ” ë‚ ì§œ í•„í„°ê°€ ì—†ì–´ì„œ, í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ìµœëŒ€í•œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
- ìµœëŒ€ start=1000ê¹Œì§€ ê°€ëŠ¥ (API ì œí•œ)
- í‚¤ì›Œë“œë‹¹ 1000ê°œ â†’ 11ê°œ í‚¤ì›Œë“œ Ã— 1000 = ìµœëŒ€ 11000ê°œ (ì¤‘ë³µ ì œê±° í›„ ë” ì ìŒ)

Usage:
    cd data-pipeline && source venv/bin/activate && cd ..
    python scripts/collect_historical.py
"""

import asyncio
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "data-pipeline"))
from dotenv import load_dotenv

load_dotenv(os.path.join(os.path.dirname(__file__), "..", ".env"))

from src.collectors import NaverNewsCollector
from src.embeddings import KoSRoBERTaEmbedding
from src.storage import MilvusClient
from src.processors import TextSplitter, Deduplicator


async def collect_historical(max_pages_per_keyword: int = 10):
    """
    ê³¼ê±° ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘

    Args:
        max_pages_per_keyword: í‚¤ì›Œë“œë‹¹ í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€ = 100ê°œ, max 10)
    """
    print("\n" + "=" * 60)
    print(f"ê³¼ê±° ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ (í‚¤ì›Œë“œë‹¹ {max_pages_per_keyword}í˜ì´ì§€)")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now()}")
    print("=" * 60)

    # í‚¤ì›Œë“œ ëª©ë¡
    keywords = [
        "ê²½ì œ",
        "ì¦ì‹œ",
        "ì£¼ì‹ì‹œì¥",
        "ì½”ìŠ¤í”¼",
        "ì½”ìŠ¤ë‹¥",
        "ì‚¼ì„±ì „ì",
        "SKí•˜ì´ë‹‰ìŠ¤",
        "ê¸ˆìœµ",
        "íˆ¬ì",
        "í™˜ìœ¨",
        "ê¸ˆë¦¬",
    ]

    collector = NaverNewsCollector()
    embedder = KoSRoBERTaEmbedding()
    storage = MilvusClient()
    splitter = TextSplitter()
    deduplicator = Deduplicator()

    all_items = []
    seen_urls = set()

    try:
        # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
        print("\nğŸ“° Step 1: ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        for keyword in keywords:
            print(f"  ìˆ˜ì§‘ ì¤‘: {keyword}", end="", flush=True)
            keyword_count = 0

            for page in range(max_pages_per_keyword):
                start = page * 100 + 1
                if start > 1000:  # API ì œí•œ
                    break

                items = await collector.search(
                    keyword, display=100, start=start, sort="date"
                )

                for item in items:
                    if item.url not in seen_urls:
                        seen_urls.add(item.url)
                        all_items.append(item)
                        keyword_count += 1

                print(".", end="", flush=True)

            print(f" +{keyword_count}ê°œ (ì´ {len(all_items)}ê°œ)")

        print(f"\n  âœ“ ì´ ìˆ˜ì§‘: {len(all_items)}ê°œ")

        # 2. Milvus ì—°ê²° ë° ì¤‘ë³µ ì²´í¬
        print("\nğŸ” Step 2: ì¤‘ë³µ ì²´í¬...")
        storage.connect()
        storage.create_collection()

        existing_count = storage.count()
        print(f"  ê¸°ì¡´ ë°ì´í„°: {existing_count}ê°œ")

        # URL ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê¸°ì¡´ DBì™€ ë¹„êµ)
        new_items = []
        for item in all_items:
            if not storage.check_url_exists(item.url):
                new_items.append(item)

        print(f"  ì‹ ê·œ ë°ì´í„°: {len(new_items)}ê°œ")

        if not new_items:
            print("\n  â„¹ï¸ ì‹ ê·œ ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ.")
            return {"collected": len(all_items), "new": 0}

        # 3. ì„ë² ë”© ìƒì„±
        print("\nğŸ§  Step 3: ì„ë² ë”© ìƒì„±...")
        texts = [item.full_text for item in new_items]
        embeddings = embedder.embed_batch(texts)
        print(f"  âœ“ {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„±")

        # 4. Milvus ì €ì¥
        print("\nğŸ’¾ Step 4: Milvus ì €ì¥...")
        # ì„ë² ë”©ì´ numpy arrayì¸ ê²½ìš° tolist(), ì´ë¯¸ listë©´ ê·¸ëŒ€ë¡œ
        embedding_list = [e.tolist() if hasattr(e, "tolist") else e for e in embeddings]
        ids = storage.insert(
            embeddings=embedding_list,
            texts=texts,
            titles=[item.title for item in new_items],
            published_dates=[item.published_at for item in new_items],
            urls=[item.url for item in new_items],
        )

        final_count = storage.count()
        print(f"  âœ“ {len(ids)}ê°œ ì €ì¥ ì™„ë£Œ")
        print(f"  ì´ ë°ì´í„°: {final_count}ê°œ")

        result = {
            "collected": len(all_items),
            "new": len(new_items),
            "stored": len(ids),
            "total": final_count,
        }

    finally:
        await collector.close()
        storage.disconnect()

    print("\n" + "=" * 60)
    print(f"ì™„ë£Œ ì‹œê°„: {datetime.now()}")
    print("=" * 60)

    return result


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ê³¼ê±° ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘")
    parser.add_argument(
        "--pages", type=int, default=5, help="í‚¤ì›Œë“œë‹¹ í˜ì´ì§€ ìˆ˜ (1-10)"
    )
    args = parser.parse_args()

    pages = min(max(args.pages, 1), 10)  # 1-10 ë²”ìœ„
    result = asyncio.run(collect_historical(max_pages_per_keyword=pages))

    print(f"\nğŸ“Š ê²°ê³¼: {result}")
